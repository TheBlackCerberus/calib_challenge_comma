{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)\n",
    "\n",
    "image_names = [\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0001.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0002.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0003.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0004.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0005.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0006.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0007.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0008.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0009.png\",\n",
    "    \"/mnt/persistent-data/calib_challenge/vggt/1/frames/frame_0010.png\",\n",
    "]\n",
    "\n",
    "images = load_and_preprocess_images(image_names).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=dtype):\n",
    "        # Predict attributes including cameras, depth maps, and point maps.\n",
    "        predictions = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_enc_data = predictions[\"pose_enc\"]\n",
    "world_points = predictions[\"world_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Auto-estimate vanishing point ===\n",
      "=== VGGT Relative Motion (radians) ===\n",
      "Pitch range: -0.023074 to 0.000143\n",
      "Yaw range: -0.009259 to 0.000724\n",
      "First frame: Pitch=0.000143, Yaw=0.000382\n",
      "\n",
      "=== Estimating Vanishing Point from Ground Truth ===\n",
      "Estimated vanishing point: [610.32, 406.66]\n",
      "\n",
      "=== Camera Mounting Angles (radians) ===\n",
      "Pitch offset: 0.033317 rad (1.909째)\n",
      "Yaw offset: 0.031110 rad (1.782째)\n",
      "\n",
      "=== Final Camera Pose (radians) ===\n",
      "Pitch: [0.03346064 0.03250136 0.02902086 0.02677931 0.02598308 0.01893042\n",
      " 0.01561101 0.01213785 0.01024308 0.01103576]\n",
      "Yaw: [0.03149206 0.03160299 0.03183388 0.03058257 0.02805724 0.02650303\n",
      " 0.024784   0.02391155 0.02343872 0.02185073]\n",
      "\n",
      "First frame final: Pitch=0.033461, Yaw=0.031492\n",
      "\n",
      "=== Comparison with Ground Truth ===\n",
      "GT Pitch:    [0.03346066 0.03332005 0.03326382 0.03328    0.03333414 0.03341528\n",
      " 0.03344653 0.03328151 0.03323636 0.03322041]\n",
      "GT Yaw:      [0.03149205 0.0313172  0.0312028  0.03122725 0.03096299 0.03096142\n",
      " 0.03097768 0.03089277 0.03106457 0.03105538]\n",
      "Final Pitch: [0.03346064 0.03250136 0.02902086 0.02677931 0.02598308 0.01893042\n",
      " 0.01561101 0.01213785 0.01024308 0.01103576]\n",
      "Final Yaw:   [0.03149206 0.03160299 0.03183388 0.03058257 0.02805724 0.02650303\n",
      " 0.024784   0.02391155 0.02343872 0.02185073]\n",
      "Difference Pitch:  [1.87390421e-08 8.18684712e-04 4.24295479e-03 6.50069637e-03\n",
      " 7.35106709e-03 1.44848628e-02 1.78355196e-02 2.11436622e-02\n",
      " 2.29932818e-02 2.21846510e-02]\n",
      "Difference Yaw:    [1.16221286e-08 2.85787996e-04 6.31084628e-04 6.44681624e-04\n",
      " 2.90575383e-03 4.45838504e-03 6.19367658e-03 6.98122013e-03\n",
      " 7.62584548e-03 9.20464556e-03]\n",
      "Max pitch error: 0.022993 rad (1.317째)\n",
      "Max yaw error: 0.009205 rad (0.527째)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "\n",
    "def extract_pitch_yaw_radians(\n",
    "    pose_enc_data: torch.Tensor, \n",
    "    image_size_hw: tuple[int, int]\n",
    ") -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Extract pitch/yaw in RADIANS using VGGT's native utilities\n",
    "    \n",
    "    Args:\n",
    "        pose_enc_data: VGGT pose encoding tensor\n",
    "        image_size_hw: Tuple of (height, width) in pixels\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (pitch_angles, yaw_angles) as numpy arrays in radians\n",
    "    \"\"\"\n",
    "    # Use VGGT's native conversion\n",
    "    extrinsics, intrinsics = pose_encoding_to_extri_intri(\n",
    "        pose_enc_data,\n",
    "        image_size_hw=image_size_hw,\n",
    "        pose_encoding_type=\"absT_quaR_FoV\"\n",
    "    )\n",
    "    \n",
    "    pitch_angles: list[float] = []\n",
    "    yaw_angles: list[float] = []\n",
    "    \n",
    "    batch_idx: int = 0  \n",
    "    num_frames: int = extrinsics.shape[1]\n",
    "    \n",
    "    for frame_idx in range(num_frames):\n",
    "        # Get rotation matrix (3x3 part of extrinsic)\n",
    "        R: NDArray[np.float64] = extrinsics[batch_idx, frame_idx, :3, :3].cpu().numpy()\n",
    "        \n",
    "        # Euler angles from rotation matrix (IN RADIANS)\n",
    "        pitch: float = np.arcsin(-R[2, 0])  # Returns radians\n",
    "        yaw: float = np.arctan2(R[1, 0], R[0, 0])  # Returns radians\n",
    "        \n",
    "        pitch_angles.append(pitch)\n",
    "        yaw_angles.append(yaw)\n",
    "    \n",
    "    return np.array(pitch_angles), np.array(yaw_angles)\n",
    "\n",
    "def calculate_camera_mounting_from_vp(\n",
    "    vp_coords: list[float], \n",
    "    camera_intrinsics: NDArray[np.float32]\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate camera mounting angles from vanishing point in RADIANS\n",
    "    \n",
    "    Args:\n",
    "        vp_coords: [x, y] vanishing point coordinates in pixels\n",
    "        camera_intrinsics: 3x3 camera intrinsic matrix\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (pitch_offset, yaw_offset) in RADIANS\n",
    "    \"\"\"\n",
    "    # Normalize vanishing point using camera intrinsics\n",
    "    intrinsics_inv: NDArray[np.float64] = np.linalg.inv(camera_intrinsics)\n",
    "    vp_homogeneous: NDArray[np.float64] = np.array([vp_coords[0], vp_coords[1], 1.0])\n",
    "    vp_normalized: NDArray[np.float64] = intrinsics_inv @ vp_homogeneous\n",
    "    \n",
    "    # Calculate mounting angles (IN RADIANS)\n",
    "    yaw_offset: float = np.arctan(vp_normalized[0])\n",
    "    pitch_offset: float = -np.arctan(vp_normalized[1] * np.cos(yaw_offset))\n",
    "    \n",
    "    return pitch_offset, yaw_offset\n",
    "\n",
    "def find_vanishing_point_from_gt(\n",
    "    gt_pitch_rad: NDArray[np.float64] | float, \n",
    "    gt_yaw_rad: NDArray[np.float64] | float, \n",
    "    vggt_pitch_rad: NDArray[np.float64], \n",
    "    vggt_yaw_rad: NDArray[np.float64], \n",
    "    camera_intrinsics: NDArray[np.float32]\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Reverse-engineer vanishing point from ground truth data\n",
    "    \n",
    "    Args:\n",
    "        gt_pitch_rad: Ground truth pitch in radians (array or single value)\n",
    "        gt_yaw_rad: Ground truth yaw in radians (array or single value)  \n",
    "        vggt_pitch_rad: VGGT relative pitch in radians\n",
    "        vggt_yaw_rad: VGGT relative yaw in radians\n",
    "        camera_intrinsics: 3x3 camera intrinsic matrix\n",
    "    \n",
    "    Returns:\n",
    "        List of [x, y] vanishing point coordinates in pixels\n",
    "    \"\"\"\n",
    "    # Calculate required offset (use first frame or average)\n",
    "    if isinstance(gt_pitch_rad, np.ndarray):\n",
    "        target_pitch_offset: float = gt_pitch_rad[0] - vggt_pitch_rad[0]\n",
    "        target_yaw_offset: float = gt_yaw_rad[0] - vggt_yaw_rad[0]\n",
    "    else:\n",
    "        target_pitch_offset = gt_pitch_rad - vggt_pitch_rad[0]\n",
    "        target_yaw_offset = gt_yaw_rad - vggt_yaw_rad[0]\n",
    "    \n",
    "    # Reverse the vanishing point calculation\n",
    "    # From: yaw_offset = arctan(vp_norm[0])\n",
    "    # To: vp_norm[0] = tan(yaw_offset)\n",
    "    vp_norm_x: float = np.tan(target_yaw_offset)\n",
    "    \n",
    "    # From: pitch_offset = -arctan(vp_norm[1] * cos(yaw_offset))\n",
    "    # To: vp_norm[1] = -tan(pitch_offset) / cos(yaw_offset)\n",
    "    vp_norm_y: float = -np.tan(target_pitch_offset) / np.cos(target_yaw_offset)\n",
    "    \n",
    "    # Convert normalized coordinates back to pixel coordinates\n",
    "    vp_normalized: NDArray[np.float64] = np.array([vp_norm_x, vp_norm_y, 1.0])\n",
    "    vp_pixel: NDArray[np.float64] = camera_intrinsics @ vp_normalized\n",
    "    \n",
    "    return [float(vp_pixel[0]), float(vp_pixel[1])]\n",
    "\n",
    "def complete_camera_calibration(\n",
    "    pose_enc_data: torch.Tensor, \n",
    "    image_size_hw: tuple[int, int], \n",
    "    vp_coords: list[float] | None = None, \n",
    "    gt_data: dict[str, NDArray[np.float64]] | None = None\n",
    ") -> tuple[NDArray[np.float64], NDArray[np.float64], dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Complete camera calibration workflow combining VGGT + vanishing point\n",
    "    \n",
    "    Args:\n",
    "        pose_enc_data: VGGT pose encoding output tensor\n",
    "        image_size_hw: (height, width) tuple\n",
    "        vp_coords: [x, y] vanishing point coordinates (if known)\n",
    "        gt_data: Dictionary with 'pitch' and 'yaw' keys containing ground truth data in radians\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_pitch_rad, final_yaw_rad, camera_mounting_info)\n",
    "    \"\"\"\n",
    "    HEIGHT, WIDTH = image_size_hw\n",
    "    \n",
    "    # Camera intrinsics based on the comma ai readme\n",
    "    FOCAL_LENGTH: float = 910.0\n",
    "    camera_intrinsics: NDArray[np.float32] = np.array([\n",
    "        [FOCAL_LENGTH, 0, WIDTH/2.0],\n",
    "        [0, FOCAL_LENGTH, HEIGHT/2.0],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    # Extract VGGT relative motion\n",
    "    vggt_pitch_rad, vggt_yaw_rad = extract_pitch_yaw_radians(pose_enc_data, image_size_hw)\n",
    "    \n",
    "    print(\"=== VGGT Relative Motion (radians) ===\")\n",
    "    print(f\"Pitch range: {vggt_pitch_rad.min():.6f} to {vggt_pitch_rad.max():.6f}\")\n",
    "    print(f\"Yaw range: {vggt_yaw_rad.min():.6f} to {vggt_yaw_rad.max():.6f}\")\n",
    "    print(f\"First frame: Pitch={vggt_pitch_rad[0]:.6f}, Yaw={vggt_yaw_rad[0]:.6f}\")\n",
    "    \n",
    "    # Determine vanishing point\n",
    "    if vp_coords is None and gt_data is not None:\n",
    "        print(\"\\n=== Estimating Vanishing Point from Ground Truth ===\")\n",
    "        vp_coords = find_vanishing_point_from_gt(\n",
    "            gt_data['pitch'], gt_data['yaw'], \n",
    "            vggt_pitch_rad, vggt_yaw_rad, \n",
    "            camera_intrinsics\n",
    "        )\n",
    "        print(f\"Estimated vanishing point: [{vp_coords[0]:.2f}, {vp_coords[1]:.2f}]\")\n",
    "    \n",
    "    # Calculate camera mounting offset\n",
    "    if vp_coords is None:\n",
    "        raise ValueError(\"Either vp_coords or gt_data must be provided\")\n",
    "        \n",
    "    pitch_offset_rad, yaw_offset_rad = calculate_camera_mounting_from_vp(vp_coords, camera_intrinsics)\n",
    "    \n",
    "    print(f\"\\n=== Camera Mounting Angles (radians) ===\")\n",
    "    print(f\"Pitch offset: {pitch_offset_rad:.6f} rad ({np.degrees(pitch_offset_rad):.3f}째)\")\n",
    "    print(f\"Yaw offset: {yaw_offset_rad:.6f} rad ({np.degrees(yaw_offset_rad):.3f}째)\")\n",
    "    \n",
    "    # Combine mounting + relative motion\n",
    "    final_pitch_rad: NDArray[np.float64] = vggt_pitch_rad + pitch_offset_rad\n",
    "    final_yaw_rad: NDArray[np.float64] = vggt_yaw_rad + yaw_offset_rad\n",
    "    \n",
    "    print(f\"\\n=== Final Camera Pose (radians) ===\")\n",
    "    print(f\"Pitch: {final_pitch_rad}\")\n",
    "    print(f\"Yaw: {final_yaw_rad}\")\n",
    "    print(f\"\\nFirst frame final: Pitch={final_pitch_rad[0]:.6f}, Yaw={final_yaw_rad[0]:.6f}\")\n",
    "    \n",
    "    mounting_info: dict[str, Any] = {\n",
    "        'vanishing_point': vp_coords,\n",
    "        'pitch_offset_rad': pitch_offset_rad,\n",
    "        'yaw_offset_rad': yaw_offset_rad,\n",
    "        'pitch_offset_deg': np.degrees(pitch_offset_rad),\n",
    "        'yaw_offset_deg': np.degrees(yaw_offset_rad),\n",
    "        'camera_intrinsics': camera_intrinsics\n",
    "    }\n",
    "    \n",
    "    return final_pitch_rad, final_yaw_rad, mounting_info\n",
    "\n",
    "def run_calibration_example(\n",
    "    pose_enc_data: torch.Tensor\n",
    ") -> tuple[NDArray[np.float64], NDArray[np.float64], dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Example usage with your VGGT pose_enc_data\n",
    "    \n",
    "    Args:\n",
    "        pose_enc_data: VGGT pose encoding tensor\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (final_pitch, final_yaw, mounting_info)\n",
    "    \"\"\"\n",
    "    HEIGHT: int = 874\n",
    "    WIDTH: int = 1164\n",
    "    FOCAL_LENGTH: float = 910.0 # TODO: use later properly\n",
    "    \n",
    "    gt_pitch_rad: NDArray[np.float64] = np.array([\n",
    "        3.346066188150387949e-02, 3.332004697594769665e-02, 3.326381557788743448e-02,\n",
    "        3.328000156359077477e-02, 3.333414362855208202e-02, 3.341528307294515387e-02,\n",
    "        3.344653297968317590e-02, 3.328151290750939323e-02, 3.323636234017846025e-02,\n",
    "        3.322040813271374959e-02\n",
    "    ])\n",
    "\n",
    "    gt_yaw_rad: NDArray[np.float64] = np.array([\n",
    "        3.149205029088487234e-02, 3.131719816086165481e-02, 3.120279874728593833e-02,\n",
    "        3.122725488847126821e-02, 3.096299378275137182e-02, 3.096141898069740273e-02,\n",
    "        3.097767903556855607e-02, 3.089276518450867828e-02, 3.106456737090537018e-02,\n",
    "        3.105538050544190062e-02\n",
    "    ])\n",
    "    \n",
    "    gt_data: dict[str, NDArray[np.float64]] = {'pitch': gt_pitch_rad, 'yaw': gt_yaw_rad}\n",
    "    \n",
    "    print(\"=== Auto-estimate vanishing point ===\")\n",
    "    final_pitch1, final_yaw1, info1 = complete_camera_calibration(\n",
    "        pose_enc_data, (HEIGHT, WIDTH), gt_data=gt_data\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Comparison with Ground Truth ===\")\n",
    "    print(f\"GT Pitch:    {gt_pitch_rad}\")\n",
    "    print(f\"GT Yaw:      {gt_yaw_rad}\")\n",
    "    print(f\"Final Pitch: {final_pitch1}\")\n",
    "    print(f\"Final Yaw:   {final_yaw1}\")\n",
    "    print(f\"Difference Pitch:  {np.abs(gt_pitch_rad - final_pitch1)}\")\n",
    "    print(f\"Difference Yaw:    {np.abs(gt_yaw_rad - final_yaw1)}\")\n",
    "    print(f\"Max pitch error: {np.max(np.abs(gt_pitch_rad - final_pitch1)):.6f} rad ({np.degrees(np.max(np.abs(gt_pitch_rad - final_pitch1))):.3f}째)\")\n",
    "    print(f\"Max yaw error: {np.max(np.abs(gt_yaw_rad - final_yaw1)):.6f} rad ({np.degrees(np.max(np.abs(gt_yaw_rad - final_yaw1))):.3f}째)\")\n",
    "    \n",
    "    return final_pitch1, final_yaw1, info1\n",
    "\n",
    "final_pitch, final_yaw, mounting_info = run_calibration_example(pose_enc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_vp = [582.10, 436.96]\n",
      "gt_vp (avg) = [573.37, 439.37]\n",
      "\n",
      "All frames:\n",
      "Frame 0: [582.10, 436.96]\n",
      "Frame 1: [581.43, 438.53]\n",
      "Frame 2: [578.98, 438.86]\n",
      "Frame 3: [577.40, 439.28]\n",
      "Frame 4: [576.81, 439.35]\n",
      "Frame 5: [571.55, 439.85]\n",
      "Frame 6: [569.12, 438.88]\n",
      "Frame 7: [566.30, 440.06]\n",
      "Frame 8: [564.67, 440.72]\n",
      "Frame 9: [565.32, 441.26]\n"
     ]
    }
   ],
   "source": [
    "# Source links: \n",
    "# https://github.com/commaai/openpilot/blob/c460f5150f961ef77b8057c1fe9532086b9768dd/common/transformations/camera.py#L99\n",
    "# https://thomasfermi.github.io/Algorithms-for-Automated-Driving/CameraCalibration/VanishingPointCameraCalibration.html?highlight=vanishing%20point#vanishing-point-method\n",
    "\n",
    "\n",
    "def get_vanishing_point_simple(\n",
    "    pose_enc_data: torch.Tensor, \n",
    "    image_size_hw: tuple[int, int]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Simple function to get forward direction vanishing point (lane lines).\n",
    "    Returns [u, v] coordinates.\n",
    "    \n",
    "    Args:\n",
    "        pose_enc_data: VGGT pose encoding tensor\n",
    "        image_size_hw: Tuple of (height, width) in pixels\n",
    "        \n",
    "    Returns:\n",
    "        Vanishing points tensor of shape [B, S, 2] where B=batch, S=sequence\n",
    "    \"\"\"\n",
    "    extrinsics, intrinsics = pose_encoding_to_extri_intri(\n",
    "        pose_enc_data,\n",
    "        image_size_hw=image_size_hw,\n",
    "        pose_encoding_type=\"absT_quaR_FoV\",\n",
    "        build_intrinsics=True\n",
    "    )\n",
    "    \n",
    "    # # Forward direction only (lane lines)\n",
    "    # forward_dir = torch.tensor([0.0, 0.0, 1.0], device=pose_enc_data.device)\n",
    "    \n",
    "    B: int\n",
    "    S: int\n",
    "    B, S = extrinsics.shape[:2]\n",
    "    \n",
    "    # Extract rotation matrices [B, S, 3, 3]\n",
    "    R: torch.Tensor = extrinsics[..., :3]\n",
    "    \n",
    "    # Transform direction: R * [0, 0, 1] \n",
    "    rotated_dir: torch.Tensor = R[..., :, 2]  # [B, S, 3] \n",
    "    \n",
    "    # Project: K * rotated_dir\n",
    "    projected: torch.Tensor = torch.bmm(\n",
    "        intrinsics.view(-1, 3, 3),\n",
    "        rotated_dir.view(-1, 3, 1)\n",
    "    ).squeeze(-1).view(B, S, 3)\n",
    "    \n",
    "    # Convert to pixel coordinates [B, S, 2]\n",
    "    vp: torch.Tensor = projected[..., :2] / projected[..., 2:3]\n",
    "    \n",
    "    return vp\n",
    "\n",
    "\n",
    "# Usage example with types\n",
    "vanishing_points: torch.Tensor = get_vanishing_point_simple(pose_enc_data, (874, 1164))\n",
    "\n",
    "gt_vp: NDArray[np.float32] = vanishing_points[0, 0].cpu().numpy()\n",
    "print(f\"gt_vp = [{gt_vp[0]:.2f}, {gt_vp[1]:.2f}]\")\n",
    "\n",
    "gt_vp_avg: NDArray[np.float32] = vanishing_points[0].mean(dim=0).cpu().numpy()\n",
    "print(f\"gt_vp (avg) = [{gt_vp_avg[0]:.2f}, {gt_vp_avg[1]:.2f}]\")\n",
    "\n",
    "print(\"\\nAll frames:\")\n",
    "for i in range(vanishing_points.shape[1]):\n",
    "    vp: NDArray[np.float32] = vanishing_points[0, i].cpu().numpy()\n",
    "    print(f\"Frame {i}: [{vp[0]:.2f}, {vp[1]:.2f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
